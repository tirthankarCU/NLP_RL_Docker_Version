{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10808,"status":"ok","timestamp":1683891133128,"user":{"displayName":"DEL Lab","userId":"05361345683290158702"},"user_tz":360},"id":"sw5HrcjQYRAc","outputId":"b1480949-d16f-455a-bbcc-c2c665009a1a"},"outputs":[],"source":["!pip3 install virtualenv\n","!virtualenv -p python3.8.10 python38\n","\n","!source /content/drive/MyDrive/Colab/RL_NLP/python38/bin/activate\n","!python --version"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18823,"status":"ok","timestamp":1683891168578,"user":{"displayName":"DEL Lab","userId":"05361345683290158702"},"user_tz":360},"id":"keLU9H2OZotC","outputId":"ef2e6445-75dd-4f8d-86c2-fa86b5e0d5bc"},"outputs":[],"source":["from google.colab import drive\n","from os.path import join  \n","import sys\n","\n","def mount_drive(ROOT):\n","    drive.mount(ROOT, force_remount=True)\n","\n","ROOT = '/content/drive'\n","mount_drive(ROOT)\n","\n","MY_GOOGLE_DRIVE_PATH = 'MyDrive/RL_NLP' \n","PROJECT_PATH = join(ROOT, MY_GOOGLE_DRIVE_PATH)\n","print(\"PROJECT_PATH: \", PROJECT_PATH)   \n","%cd \"{PROJECT_PATH}\"\n","sys.path.insert(0, PROJECT_PATH)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":26095,"status":"ok","timestamp":1683891194670,"user":{"displayName":"DEL Lab","userId":"05361345683290158702"},"user_tz":360},"id":"0MNuM_mfchFj","outputId":"49881c7b-24c7-4222-c447-a1805dc05950"},"outputs":[],"source":["!pip install -r \"{PROJECT_PATH}/metadata/requirements.txt\""]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"jRmhh_o0OgbT"},"source":["### RUN IF GYM IS NOT THERE IN DRIVE.\n","\n","!git clone https://github.com/Farama-Foundation/gym-examples\n","\n","%cd gym-examples\n","\n","!pip install -e .\n","\n","%cd .."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"08zOvoU7OgbV"},"source":["# RUN FROM HERE."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xG2FfgrYeLGS"},"outputs":[],"source":["import numpy as np \n","import pandas as pd \n","import time\n","import utils as U\n","import model as M\n","import torch \n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import matplotlib.pyplot as plt \n","import json\n","import copy"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":124367,"status":"ok","timestamp":1683891328761,"user":{"displayName":"DEL Lab","userId":"05361345683290158702"},"user_tz":360},"id":"v59dypnDOgbX","outputId":"48d76d0e-a3f8-48ee-b300-b172e6fda882"},"outputs":[],"source":["!pip install -e ."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5835,"status":"ok","timestamp":1683891334590,"user":{"displayName":"DEL Lab","userId":"05361345683290158702"},"user_tz":360},"id":"M0FpJmvmOgbY","outputId":"4a067427-320f-49c6-a3d0-5fc768db50f2"},"outputs":[],"source":["import gym\n","%cd gym-examples\n","import gym_examples\n","%cd ..\n","import numpy as np\n","\n","episodes=2\n","env = gym.make('gym_examples/RlNlpWorld-v0',render_mode=\"rgb_array\")"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"qv5AAhIaOgbY"},"source":["### Random Policy."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"61MYl-tUOgbZ"},"outputs":[],"source":["def policy(observation):\n","    return np.random.randint(0,6)\n","\n","dbg=True\n","episodes=1\n","env = gym.make('gym_examples/RlNlpWorld-v0',render_mode=\"rgb_array\")\n","for _ in range(episodes):\n","    cumulative_reward,steps=0,0\n","    observation = env.reset(seed=42)\n","    cnt,mx_iter=0,10\n","    while mx_iter<steps:\n","        action = policy(observation)  # User-defined policy function\n","        observation, reward, terminated, info = env.step(action)\n","        cumulative_reward+=reward\n","        steps+=1\n","        if dbg==True:\n","            print(f'cumulative_reward {cumulative_reward}; action {action}')\n","        if terminated:\n","            break\n","    print(f'Cumulative Reward ~ {cumulative_reward}; TimeTaken ~ {steps}')\n","env.close()"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"BgkqTy7qaOF3"},"source":["### Delete Files."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QKhpu7uBaM8J"},"outputs":[],"source":["import os\n","\n","def delete(state):\n","    file_path=f'/content/drive/MyDrive/RL_NLP/save_state/{state}.json'\n","    try:\n","        U.delete(state)\n","        os.unlink(file_path)\n","    except Exception as e:\n","        print(f'Error deleting {file_path}: {e}')"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"IOU1LxNUOgbZ"},"source":["### Q-Learning w/o NLP. (NAIVE MODEL)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f6WX6YpyOgba"},"outputs":[],"source":["!rm -rf \"{PROJECT_PATH}/save_state\"\n","!mkdir \"{PROJECT_PATH}/save_state\"\n","!mkdir \"{PROJECT_PATH}/results\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I9I2zokVhaOS"},"outputs":[],"source":["env = gym.make('gym_examples/RlNlpWorld-v0',mx_timeSteps=100,render_mode=\"rgb_array\")\n","num_actions=6\n","update_after_actions=2**8\n","update_target_network=2**10\n","frame_cnt,epsilon_greedy=0,20000\n","episodes,mxSteps=100000,100\n","episode_reward_hist,episode_avg_reward=[],[]\n","epsilon,epsilon_min,epsilon_max=1.0,0.005,1.0 \n","epsilon_interval = (\n","    epsilon_max - epsilon_min\n",") \n","epsilon_greedy_frames = 100000.0\n","batch_size=2**7\n","action=-1\n","replay_mem={'state':[],'next_state':[],'action':[],'reward':[],'small_n_state':[]}\n","mx_replay_memory=100000\n","gamma=0.9\n","# MODEL\n","save_step=5000;load_from_checkpoint=False\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","epochs=10\n","first=True"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jsWmaOEPDBiJ"},"outputs":[],"source":["episodes=1000\n","load_from_checkpoint=True"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"ilhGsmWOCEvP"},"source":["#### Run this cell multiple times in short episodes because google colab throws error (disconnects with drive) if run for long.\n","Keep the cnt here -> 1"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"Kx_noYF2Ogba","outputId":"603096bf-48ce-44b0-8b04-f036f90d001e"},"outputs":[],"source":["model=M.NNModel().to(device)\n","model_traget=M.NNModel().to(device)\n","if load_from_checkpoint==True:\n","    model.load_state_dict(torch.load('results/model.ml'))\n","    model_traget.load_state_dict(torch.load('results/model.ml'))\n","    episode_reward_hist=np.load(f'results/ep_tot.npy').tolist()\n","    episode_avg_reward=np.load(f'results/ep_avg.npy').tolist()\n","    replay_mem['state']=np.load(f'results/state.npy').tolist()\n","    replay_mem['next_state']=np.load(f'results/next_state.npy').tolist()\n","    # replay_mem['small_n_state']=np.load(f'results/small_n_state.npy').tolist()\n","    replay_mem['action']=np.load(f'results/action.npy').tolist()\n","    replay_mem['reward']=np.load(f'results/reward.npy').tolist()\n","    file_read=open('results/frame_cnt.dat','r')\n","    frame_cnt=int(file_read.read())\n","    file_read.close()\n","    file_read=open('results/epsilon.dat','r')\n","    epsilon=float(file_read.read())\n","    file_read.close()\n","if first:   \n","    load_from_checkpoint=True\n","    first=False\n","optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.5)\n","# frame_cnt\n","# epsilon\n","for _ in range(episodes):\n","    episode_reward=0\n","    state = env.reset(seed=42)\n","    for __ in range(mxSteps):\n","        frame_cnt+=1\n","        epsilon -= epsilon_interval / epsilon_greedy_frames\n","        epsilon = max(epsilon, epsilon_min)\n","        U.phi(state);replay_mem['state'].append(U.save(state))\n","        if frame_cnt<=epsilon_greedy or epsilon>np.random.rand(1)[0]:\n","            action=np.random.choice(num_actions)\n","            # with open('my_dict.json', 'w') as f:\n","            #     U.phi(state)\n","            #     state[\"visual\"]=state[\"visual\"].tolist()\n","            #     json.dump(state,f)\n","        else:\n","            action=(M.predict(model,[U.get(replay_mem['state'][-1])],device))[1]\n","            action=action.item()\n","        next_state,reward,terminated,info=env.step(action)\n","        episode_reward+=reward \n","        # temp_next_state=copy.deepcopy(next_state) #to save small image.\n","        state=copy.deepcopy(next_state)\n","        U.phi(next_state);replay_mem['next_state'].append(U.save(next_state))\n","        # U.phiXtra(temp_next_state);replay_mem['small_n_state'].append(U.save(temp_next_state))\n","        replay_mem['action'].append(action)\n","        replay_mem['reward'].append(reward)\n","        if (frame_cnt%save_step==0): # SAVE model temporarily so progress is not lost.\n","            print(f'Saving ...')\n","            np.save(f'results/ep_tot.npy',np.array(episode_reward_hist))\n","            np.save(f'results/ep_avg.npy',np.array(episode_avg_reward))\n","            np.save(f'results/state.npy',np.array(replay_mem['state']))\n","            np.save(f'results/next_state.npy',np.array(replay_mem['next_state']))\n","            # np.save(f'results/small_n_state.npy',np.array(replay_mem['small_n_state']))\n","            np.save(f'results/action.npy',np.array(replay_mem['action']))\n","            np.save(f'results/reward.npy',np.array(replay_mem['reward']))\n","            file_write=open('results/frame_cnt.dat','w')\n","            file_write.write(str(frame_cnt))\n","            file_write.close()\n","            file_write=open('results/epsilon.dat','w')\n","            file_write.write(str(epsilon))\n","            file_write.close()\n","            torch.save(model.state_dict(),'results/model.ml')\n","        if frame_cnt%update_after_actions==0 and len(replay_mem['reward'])>batch_size:\n","            choices=np.random.choice( range( len(replay_mem['reward']) ), size=batch_size )\n","            STATE=[U.get(replay_mem['state'][id]) for id in choices]\n","            NEXT_STATE=[U.get(replay_mem['next_state'][id]) for id in choices]\n","            # SMALL_NEXT_STATE=[U.get(replay_mem['small_n_state'][id]) for id in choices]\n","            ACTION=torch.stack([U.oneHot(num_actions,replay_mem['action'][id]).squeeze() for id in choices])\n","            REWARD=np.array([replay_mem['reward'][id] for id in choices])\n","            IMG=np.array([_state_[\"visual\"] for _state_ in STATE])\n","            TEMP=M.predict(model_traget,NEXT_STATE,device)\n","            REWARD_T=torch.tensor(REWARD)\n","            if torch.cuda.is_available():\n","                REWARD_T=REWARD_T.to(torch.device(\"cuda:0\")) \n","            reward_true=torch.add(REWARD_T,gamma*TEMP[0][0])\n","            for epoch in range(epochs):\n","                # M.train(model,reward_true,STATE,SMALL_NEXT_STATE,ACTION,device,optimizer,type='dqn')\n","                # M.train(model,reward_true,STATE,SMALL_NEXT_STATE,ACTION,device,optimizer,type='image')\n","                M.train(model,reward_true,STATE,ACTION,device,optimizer,type='dqn')\n","                M.train(model,reward_true,STATE,ACTION,device,optimizer,type='image')\n","        if frame_cnt%update_target_network==0:\n","            print(f'Updating N/W ...{frame_cnt}')\n","            model_traget.load_state_dict(model.state_dict())\n","        if len(replay_mem['reward'])>mx_replay_memory:\n","            print('Deleting ...')\n","            delete(replay_mem['state'][:1][0]);del replay_mem['state'][:1]\n","            # delete(replay_mem['small_n_state'][:1][0]);del replay_mem['small_n_state'][:1]\n","            delete(replay_mem['next_state'][:1][0]);del replay_mem['next_state'][:1]\n","            del replay_mem['action'][:1]\n","            del replay_mem['reward'][:1]\n","        if terminated:\n","            break\n","    episode_reward_hist.append(episode_reward)\n","    episode_avg_reward.append(episode_reward/__)\n","\n","# Save Relevant Results\n","np.save('results/ep_tot.npy',np.array(episode_reward_hist))\n","np.save('results/ep_avg.npy',np.array(episode_avg_reward))\n","torch.save(model.state_dict(),'results/model.ml')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P5Bsa8n6Ogbc"},"outputs":[],"source":["# Loading Results.\n","episode_reward_hist=np.load('results/ep_tot.npy')\n","episode_avg_reward=np.load('results/ep_avg.npy')\n","# U.plot(episode_reward_hist,'Total Reward','Total Reward V/S Episodes')\n","# U.plot(episode_avg_reward,'Avg. Reward','Avg. Reward Per Step V/S Episodes')\n","model_tr=M.NNModel().to(device)\n","model_tr.load_state_dict(torch.load('results/model.ml'))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":447},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1683839051981,"user":{"displayName":"DEL Lab","userId":"05361345683290158702"},"user_tz":360},"id":"h2XExFEpn0Yk","outputId":"6c8fa358-3868-42a6-86b4-6700bdc8e0da"},"outputs":[],"source":["plt.plot(episode_reward_hist)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"2fWimtLykKvE"},"source":["### Q-Learning with NLP Fully."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21246,"status":"ok","timestamp":1683892499631,"user":{"displayName":"DEL Lab","userId":"05361345683290158702"},"user_tz":360},"id":"K-jR_OyREhiE","outputId":"42b2786b-3388-4e1c-aa25-a290c7d49ef1"},"outputs":[],"source":["!rm -rf \"{PROJECT_PATH}/save_state\"\n","!mkdir \"{PROJECT_PATH}/save_state\"\n","!mkdir \"{PROJECT_PATH}/results\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cSiJ8u46l2qK"},"outputs":[],"source":["import model_nlp as M_NLP"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1683892528397,"user":{"displayName":"DEL Lab","userId":"05361345683290158702"},"user_tz":360},"id":"_lkqyIsYZWsK","outputId":"92eaa0ba-6f1a-4065-d17f-e67b534a977e"},"outputs":[],"source":["env = gym.make('gym_examples/RlNlpWorld-v0',mx_timeSteps=50,render_mode=\"rgb_array\")\n","num_actions=6\n","update_after_actions=2**6\n","update_target_network=2**8\n","frame_cnt,epsilon_greedy=0,10000\n","episodes,mxSteps=100000,100\n","episode_reward_hist,episode_avg_reward=[],[]\n","epsilon,epsilon_min,epsilon_max=1.0,0.005,1.0 \n","epsilon_interval = (\n","    epsilon_max - epsilon_min\n",") \n","epsilon_greedy_frames = 10000.0\n","batch_size=2**7\n","action=-1\n","replay_mem={'state':[],'next_state':[],'action':[],'reward':[],'small_n_state':[]}\n","mx_replay_memory=10000\n","gamma=0.9\n","# MODEL\n","save_step=5000;load_from_checkpoint=False\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","epochs=10\n","first=True"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XPCeRTXaZXcz"},"outputs":[],"source":["episodes=1000\n","load_from_checkpoint=True"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"ShEaaovxkHpW"},"outputs":[],"source":["model=M_NLP.NNModelNLP().to(device)\n","model_traget=M_NLP.NNModelNLP().to(device)\n","if load_from_checkpoint==True:\n","    model.load_state_dict(torch.load('results/model_nlpf.ml'))\n","    model_traget.load_state_dict(torch.load('results/model_nlpf.ml'))\n","    episode_reward_hist=np.load(f'results/ep_tot_nlpf.npy').tolist()\n","    episode_avg_reward=np.load(f'results/ep_avg_nlpf.npy').tolist()\n","    replay_mem['state']=np.load(f'results/state_nlpf.npy').tolist()\n","    replay_mem['next_state']=np.load(f'results/next_state_nlpf.npy').tolist()\n","    replay_mem['action']=np.load(f'results/action_nlpf.npy').tolist()\n","    replay_mem['reward']=np.load(f'results/reward_nlpf.npy').tolist()\n","    file_read=open('results/frame_cnt_nlpf.dat','r')\n","    frame_cnt=int(file_read.read())\n","    file_read.close()\n","    file_read=open('results/epsilon_nlpf.dat','r')\n","    epsilon=float(file_read.read())\n","    file_read.close()\n","if first:   \n","    load_from_checkpoint=True\n","    first=False\n","optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.5)\n","try:\n","    for _ in range(episodes):\n","        episode_reward=0\n","        state = env.reset(seed=42)\n","        print(f'Episode no {_}')\n","        for __ in range(mxSteps):\n","            frame_cnt+=1\n","            epsilon -= epsilon_interval / epsilon_greedy_frames\n","            epsilon = max(epsilon, epsilon_min)\n","            U.phi(state);replay_mem['state'].append(U.save(state))\n","            if frame_cnt<=epsilon_greedy or epsilon>np.random.rand(1)[0]:\n","                action=np.random.choice(num_actions)\n","            else:\n","                action=(M_NLP.predict(model,[U.get(replay_mem['state'][-1])],device))[1]\n","                action=action.item()\n","            next_state,reward,terminated,info=env.step(action)\n","            episode_reward+=reward \n","            # temp_next_state=copy.deepcopy(next_state) #to save small image.\n","            state=copy.deepcopy(next_state)\n","            U.phi(next_state);replay_mem['next_state'].append(U.save(next_state))\n","            # U.phiXtra(temp_next_state);replay_mem['small_n_state'].append(U.save(temp_next_state))\n","            replay_mem['action'].append(action)\n","            replay_mem['reward'].append(reward)\n","            if frame_cnt%update_after_actions==0 and len(replay_mem['reward'])>batch_size:\n","                choices=np.random.choice( range( len(replay_mem['reward']) ), size=batch_size )\n","                STATE=[U.get(replay_mem['state'][id]) for id in choices]\n","                NEXT_STATE=[U.get(replay_mem['next_state'][id]) for id in choices]\n","                # SMALL_NEXT_STATE=[U.get(replay_mem['small_n_state'][id]) for id in choices]\n","                ACTION=torch.stack([U.oneHot(num_actions,replay_mem['action'][id]).squeeze() for id in choices])\n","                REWARD=np.array([replay_mem['reward'][id] for id in choices])\n","                IMG=np.array([_state_[\"visual\"] for _state_ in STATE])\n","                TEMP=M_NLP.predict(model_traget,NEXT_STATE,device)\n","                REWARD_T=torch.tensor(REWARD)\n","                if torch.cuda.is_available():\n","                    REWARD_T=REWARD_T.to(torch.device(\"cuda:0\")) \n","                reward_true=torch.add(REWARD_T,gamma*TEMP[0][0])\n","                for epoch in range(epochs):\n","                    # M_NLP.train(model,reward_true,STATE,SMALL_NEXT_STATE,ACTION,device,optimizer,type='dqn')\n","                    # M_NLP.train(model,reward_true,STATE,SMALL_NEXT_STATE,ACTION,device,optimizer,type='image')\n","                    M_NLP.train(model,reward_true,STATE,ACTION,device,optimizer,type='dqn')\n","                    M_NLP.train(model,reward_true,STATE,ACTION,device,optimizer,type='image')\n","            if frame_cnt%update_target_network==0:\n","                model_traget.load_state_dict(model.state_dict())\n","            if len(replay_mem['reward'])>mx_replay_memory:\n","                delete(replay_mem['state'][:1][0]);del replay_mem['state'][:1]\n","                # delete(replay_mem['small_n_state'][:1][0]);del replay_mem['small_n_state'][:1]\n","                delete(replay_mem['next_state'][:1][0]);del replay_mem['next_state'][:1]\n","                del replay_mem['action'][:1]\n","                del replay_mem['reward'][:1]\n","            if terminated:\n","                break\n","        episode_reward_hist.append(episode_reward)\n","        episode_avg_reward.append(episode_reward/__)\n","    # Save Relevant Results\n","    np.save('results/ep_tot_nlpf.npy',np.array(episode_reward_hist))\n","    np.save('results/ep_avg_nlpf.npy',np.array(episode_avg_reward))\n","    torch.save(model.state_dict(),'results/model_nlpf.ml')\n","except Exception as e:\n","    print(f'Saving ...')\n","    np.save(f'results/ep_tot_nlpf.npy',np.array(episode_reward_hist))\n","    np.save(f'results/ep_avg_nlpf.npy',np.array(episode_avg_reward))\n","    np.save(f'results/state_nlpf.npy',np.array(replay_mem['state']))\n","    np.save(f'results/next_state_nlpf.npy',np.array(replay_mem['next_state']))\n","    np.save(f'results/action_nlpf.npy',np.array(replay_mem['action']))\n","    np.save(f'results/reward_nlpf.npy',np.array(replay_mem['reward']))\n","    file_write=open('results/frame_cnt_nlpf.dat','w')\n","    file_write.write(str(frame_cnt))\n","    file_write.close()\n","    file_write=open('results/epsilon_nlpf.dat','w')\n","    file_write.write(str(epsilon))\n","    file_write.close()\n","    torch.save(model.state_dict(),'results/model_nlpf.ml')\n","    print(e)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"V7nJNTjx4_gB"},"source":["### Q-Learning with NLP. {TBD}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7-qRAsmDji6c"},"outputs":[],"source":["num_actions=6\n","update_after_actions=2**8\n","update_target_network=2**10\n","frame_cnt,epsilon_greedy=0,20000\n","episodes,mxSteps=100000,100\n","episode_reward_hist,episode_avg_reward=[],[]\n","epsilon,epsilon_min,epsilon_max=1.0,0.005,1.0 \n","epsilon_interval = (\n","    epsilon_max - epsilon_min\n",") \n","epsilon_greedy_frames = 100000.0\n","batch_size=2**7\n","action=-1\n","replay_mem={'state':[],'next_state':[],'action':[],'reward':[],'small_n_state':[]}\n","mx_replay_memory=100000\n","gamma=0.9\n","# MODEL\n","save_step=5000;load_from_checkpoint=False\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","epochs=10\n","first=True"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W1QZzYbCjmL9"},"outputs":[],"source":["episodes=1000\n","load_from_checkpoint=False"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J56y_01u44b6"},"outputs":[],"source":["model=M_NLP.NNModelNLP().to(device)\n","model_traget=M_NLP.NNModelNLP().to(device)\n","if load_from_checkpoint==True:\n","    model.load_state_dict(torch.load('results/model_nlp.ml'))\n","    model_traget.load_state_dict(torch.load('results/model_nlp.ml'))\n","optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.5)\n","epochs=10\n","#### REMOVE INSTRUCTIONS SLOWLY MODEL IS NOT SO DEPENDENT ####\n","epsilon_nlp=1.0\n","epsilon_greedy_frames_nlp=100000.0\n","##############################################################\n","\n","for _ in range(episodes):\n","    episode_reward=0\n","    state = env.reset(seed=42)\n","    for __ in range(mxSteps):\n","        frame_cnt+=1\n","        epsilon -= epsilon_interval / epsilon_greedy_frames\n","        epsilon = max(epsilon, epsilon_min)\n","#### REMOVE INSTRUCTIONS SLOWLY MODEL IS NOT SO DEPENDENT ####\n","        epsilon_nlp -= epsilon_interval / epsilon_greedy_frames_nlp\n","        if epsilon_nlp<np.random.rand(1)[0]:\n","            state['text']='[PAD]'\n","##############################################################\n","        U.phi(state);replay_mem['state'].append(U.save(state))\n","        if frame_cnt<=epsilon_greedy or epsilon>np.random.rand(1)[0]:\n","            action=np.random.choice(num_actions)\n","        else:\n","            action=(M_NLP.predict(model,[U.get(replay_mem['state'][-1])],device))[1]\n","            action=action.item()\n","        next_state,reward,terminated,info=env.step(action)\n","        episode_reward+=reward \n","        temp_next_state=copy.deepcopy(next_state) #to save small image.\n","        state=copy.deepcopy(next_state)\n","        U.phi(next_state);replay_mem['next_state'].append(U.save(next_state))\n","        U.phiXtra(temp_next_state);replay_mem['small_n_state'].append(U.save(temp_next_state))\n","        replay_mem['action'].append(action)\n","        replay_mem['reward'].append(reward)\n","        if (_!=0 and _%save_step==0): # SAVE model temporarily so progress is not lost.\n","            np.save(f'results/ep_tot_nlp{_}.npy',np.array(episode_reward_hist))\n","            np.save(f'results/ep_avg_nlp{_}.npy',np.array(episode_avg_reward))\n","            torch.save(model.state_dict(),'results/model_nlp.ml')\n","        if frame_cnt%update_after_actions==0 and len(replay_mem['reward'])>batch_size:\n","            choices=np.random.choice( range( len(replay_mem['reward']) ), size=batch_size )\n","            STATE=[U.get(replay_mem['state'][id]) for id in choices]\n","            NEXT_STATE=[U.get(replay_mem['next_state'][id]) for id in choices]\n","            SMALL_NEXT_STATE=[U.get(replay_mem['small_n_state'][id]) for id in choices]\n","            ACTION=torch.stack([U.oneHot(num_actions,replay_mem['action'][id]).squeeze() for id in choices])\n","            REWARD=np.array([replay_mem['reward'][id] for id in choices])\n","            IMG=np.array([_state_[\"visual\"] for _state_ in STATE])\n","            TEMP=M_NLP.predict(model_traget,NEXT_STATE,device);\n","            REWARD_T=torch.tensor(REWARD)\n","            reward_true=torch.add(REWARD_T,gamma*TEMP[0][0])\n","            for epoch in range(epochs):\n","                M_NLP.train(model,reward_true,STATE,SMALL_NEXT_STATE,ACTION,device,optimizer,type='dqn')\n","                M_NLP.train(model,reward_true,STATE,SMALL_NEXT_STATE,ACTION,device,optimizer,type='image')\n","        if frame_cnt%update_target_network==0:\n","            model_traget.load_state_dict(model.state_dict())\n","        if len(replay_mem['reward'])>mx_replay_memory:\n","            delete(replay_mem['state'][:1][0]);del replay_mem['state'][:1]\n","            delete(replay_mem['small_n_state'][:1][0]);del replay_mem['small_n_state'][:1]\n","            delete(replay_mem['next_state'][:1][0]);del replay_mem['next_state'][:1]\n","            del replay_mem['action'][:1]\n","            del replay_mem['reward'][:1]\n","        if terminated:\n","            break\n","    \n","    episode_reward_hist.append(episode_reward)\n","    episode_avg_reward.append(episode_reward/__)\n","\n","# Save Relevant Results\n","np.save('results/ep_tot_nlp.npy',np.array(episode_reward_hist))\n","np.save('results/ep_avg_nlp.npy',np.array(episode_avg_reward))\n","torch.save(model.state_dict(),'results/model_nlp.ml')"]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","name":"","version":""},"gpuClass":"premium","kernelspec":{"display_name":"tf","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"},"vscode":{"interpreter":{"hash":"607977446fd7db0f3e2ddba643f6a09248589e32f4e48c8af2325eb4debcc38d"}}},"nbformat":4,"nbformat_minor":0}
