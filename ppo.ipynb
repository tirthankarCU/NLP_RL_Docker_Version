{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append('/NLP_RL_Docker_Version')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import time\n",
    "import utils as U\n",
    "import model as M\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt \n",
    "import json\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd NLP_RL_Docker_Version/\n",
    "!pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "%cd gym-examples\n",
    "import gym_examples\n",
    "%cd ..\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Human Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def human_policy(observation):\n",
    "    action=int(input())\n",
    "    return action\n",
    "\n",
    "dbg=True\n",
    "episodes=1\n",
    "env = gym.make('gym_examples/RlNlpWorld-v0',render_mode=\"rgb_array\")\n",
    "for _ in range(episodes):\n",
    "    cumulative_reward,steps=0,0\n",
    "    observation = env.reset(set_no=12, seed=42)\n",
    "    cnt,mx_iter=0,10\n",
    "    while steps<mx_iter:\n",
    "        print(observation['text'])\n",
    "        plt.imshow(observation['visual'])\n",
    "        plt.show()\n",
    "        action = human_policy(observation)  # User-defined policy function\n",
    "        observation, reward, terminated, info = env.step(action)\n",
    "        cumulative_reward+=reward\n",
    "        steps+=1\n",
    "        if dbg==True:\n",
    "            print(f'cumulative_reward {cumulative_reward}; action {action}')\n",
    "        if terminated:\n",
    "            break\n",
    "    print(f'Cumulative Reward ~ {cumulative_reward}; TimeTaken ~ {steps}')\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
