{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4690,
     "status": "ok",
     "timestamp": 1683768510987,
     "user": {
      "displayName": "DEL Lab",
      "userId": "05361345683290158702"
     },
     "user_tz": 360
    },
    "id": "sw5HrcjQYRAc",
    "outputId": "a85adb9f-8227-442a-e958-400a5373852b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: virtualenv in /usr/local/lib/python3.10/dist-packages (20.23.0)\n",
      "Requirement already satisfied: distlib<1,>=0.3.6 in /usr/local/lib/python3.10/dist-packages (from virtualenv) (0.3.6)\n",
      "Requirement already satisfied: filelock<4,>=3.11 in /usr/local/lib/python3.10/dist-packages (from virtualenv) (3.12.0)\n",
      "Requirement already satisfied: platformdirs<4,>=3.2 in /usr/local/lib/python3.10/dist-packages (from virtualenv) (3.3.0)\n",
      "created virtual environment CPython3.8.10.final.0-64 in 233ms\n",
      "  creator CPython3Posix(dest=/content/python38, clear=False, no_vcs_ignore=False, global=False)\n",
      "  seeder FromAppData(download=False, pip=bundle, setuptools=bundle, wheel=bundle, via=copy, app_data_dir=/root/.local/share/virtualenv)\n",
      "    added seed packages: pip==23.1.2, setuptools==67.7.2, wheel==0.40.0\n",
      "  activators BashActivator,CShellActivator,FishActivator,NushellActivator,PowerShellActivator,PythonActivator\n",
      "/bin/bash: /content/drive/MyDrive/Colab/RL_NLP/python38/bin/activate: No such file or directory\n",
      "Python 3.10.11\n"
     ]
    }
   ],
   "source": [
    "!pip3 install virtualenv\n",
    "!virtualenv -p python3.8.10 python38\n",
    "\n",
    "!source /content/drive/MyDrive/Colab/RL_NLP/python38/bin/activate\n",
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3620,
     "status": "ok",
     "timestamp": 1683768514602,
     "user": {
      "displayName": "DEL Lab",
      "userId": "05361345683290158702"
     },
     "user_tz": 360
    },
    "id": "keLU9H2OZotC",
    "outputId": "52233849-4473-4184-c417-f3cd05d9d30f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n",
      "PROJECT_PATH:  /content/drive/MyDrive/RL_NLP\n",
      "/content/drive/MyDrive/RL_NLP\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "from os.path import join  \n",
    "import sys\n",
    "\n",
    "def mount_drive(ROOT):\n",
    "    drive.mount(ROOT, force_remount=True)\n",
    "\n",
    "ROOT = '/content/drive'\n",
    "mount_drive(ROOT)\n",
    "\n",
    "MY_GOOGLE_DRIVE_PATH = 'MyDrive/RL_NLP' \n",
    "PROJECT_PATH = join(ROOT, MY_GOOGLE_DRIVE_PATH)\n",
    "print(\"PROJECT_PATH: \", PROJECT_PATH)   \n",
    "%cd \"{PROJECT_PATH}\"\n",
    "sys.path.insert(0, PROJECT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4320,
     "status": "ok",
     "timestamp": 1683768518917,
     "user": {
      "displayName": "DEL Lab",
      "userId": "05361345683290158702"
     },
     "user_tz": 360
    },
    "id": "0MNuM_mfchFj",
    "outputId": "41ef08ce-d85c-4e42-ff61-71ce22c8ee47"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: nltk==3.7 in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/RL_NLP/metadata/requirements.txt (line 1)) (3.7)\n",
      "Requirement already satisfied: pandas==1.5.3 in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/RL_NLP/metadata/requirements.txt (line 2)) (1.5.3)\n",
      "Requirement already satisfied: matplotlib==3.5 in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/RL_NLP/metadata/requirements.txt (line 3)) (3.5.0)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/RL_NLP/metadata/requirements.txt (line 4)) (4.29.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk==3.7->-r /content/drive/MyDrive/RL_NLP/metadata/requirements.txt (line 1)) (8.1.3)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk==3.7->-r /content/drive/MyDrive/RL_NLP/metadata/requirements.txt (line 1)) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk==3.7->-r /content/drive/MyDrive/RL_NLP/metadata/requirements.txt (line 1)) (2022.10.31)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk==3.7->-r /content/drive/MyDrive/RL_NLP/metadata/requirements.txt (line 1)) (4.65.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas==1.5.3->-r /content/drive/MyDrive/RL_NLP/metadata/requirements.txt (line 2)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas==1.5.3->-r /content/drive/MyDrive/RL_NLP/metadata/requirements.txt (line 2)) (2022.7.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas==1.5.3->-r /content/drive/MyDrive/RL_NLP/metadata/requirements.txt (line 2)) (1.22.4)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.5->-r /content/drive/MyDrive/RL_NLP/metadata/requirements.txt (line 3)) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.5->-r /content/drive/MyDrive/RL_NLP/metadata/requirements.txt (line 3)) (4.39.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.5->-r /content/drive/MyDrive/RL_NLP/metadata/requirements.txt (line 3)) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.5->-r /content/drive/MyDrive/RL_NLP/metadata/requirements.txt (line 3)) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.5->-r /content/drive/MyDrive/RL_NLP/metadata/requirements.txt (line 3)) (8.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.5->-r /content/drive/MyDrive/RL_NLP/metadata/requirements.txt (line 3)) (3.0.9)\n",
      "Requirement already satisfied: setuptools-scm>=4 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.5->-r /content/drive/MyDrive/RL_NLP/metadata/requirements.txt (line 3)) (7.1.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers->-r /content/drive/MyDrive/RL_NLP/metadata/requirements.txt (line 4)) (3.12.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from transformers->-r /content/drive/MyDrive/RL_NLP/metadata/requirements.txt (line 4)) (0.14.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers->-r /content/drive/MyDrive/RL_NLP/metadata/requirements.txt (line 4)) (6.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers->-r /content/drive/MyDrive/RL_NLP/metadata/requirements.txt (line 4)) (2.27.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers->-r /content/drive/MyDrive/RL_NLP/metadata/requirements.txt (line 4)) (0.13.3)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers->-r /content/drive/MyDrive/RL_NLP/metadata/requirements.txt (line 4)) (2023.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers->-r /content/drive/MyDrive/RL_NLP/metadata/requirements.txt (line 4)) (4.5.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas==1.5.3->-r /content/drive/MyDrive/RL_NLP/metadata/requirements.txt (line 2)) (1.16.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from setuptools-scm>=4->matplotlib==3.5->-r /content/drive/MyDrive/RL_NLP/metadata/requirements.txt (line 3)) (67.7.2)\n",
      "Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from setuptools-scm>=4->matplotlib==3.5->-r /content/drive/MyDrive/RL_NLP/metadata/requirements.txt (line 3)) (2.0.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->-r /content/drive/MyDrive/RL_NLP/metadata/requirements.txt (line 4)) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->-r /content/drive/MyDrive/RL_NLP/metadata/requirements.txt (line 4)) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->-r /content/drive/MyDrive/RL_NLP/metadata/requirements.txt (line 4)) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->-r /content/drive/MyDrive/RL_NLP/metadata/requirements.txt (line 4)) (3.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install -r \"{PROJECT_PATH}/metadata/requirements.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jRmhh_o0OgbT"
   },
   "source": [
    "### RUN IF GYM IS NOT THERE IN DRIVE.\n",
    "\n",
    "!git clone https://github.com/Farama-Foundation/gym-examples\n",
    "\n",
    "%cd gym-examples\n",
    "\n",
    "!pip install -e .\n",
    "\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "08zOvoU7OgbV"
   },
   "source": [
    "# RUN FROM HERE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "xG2FfgrYeLGS"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import time\n",
    "# import utils as U\n",
    "# import model as M\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt \n",
    "import json\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1158,
     "status": "ok",
     "timestamp": 1683768523610,
     "user": {
      "displayName": "DEL Lab",
      "userId": "05361345683290158702"
     },
     "user_tz": 360
    },
    "id": "v59dypnDOgbX",
    "outputId": "52bcadce-5d35-4e45-98e9-aac5c876081a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///home/trithankar-mittra/Desktop/NLP_RL_DELLAB\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: gym==0.26.0 in /home/trithankar-mittra/anaconda3/lib/python3.10/site-packages (from gym-examples==0.0.1) (0.26.0)\n",
      "Requirement already satisfied: pygame==2.1.0 in /home/trithankar-mittra/anaconda3/lib/python3.10/site-packages (from gym-examples==0.0.1) (2.1.0)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in /home/trithankar-mittra/anaconda3/lib/python3.10/site-packages (from gym==0.26.0->gym-examples==0.0.1) (0.0.8)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /home/trithankar-mittra/anaconda3/lib/python3.10/site-packages (from gym==0.26.0->gym-examples==0.0.1) (1.23.5)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /home/trithankar-mittra/anaconda3/lib/python3.10/site-packages (from gym==0.26.0->gym-examples==0.0.1) (2.0.0)\n",
      "Installing collected packages: gym-examples\n",
      "  Attempting uninstall: gym-examples\n",
      "    Found existing installation: gym-examples 0.0.1\n",
      "    Uninstalling gym-examples-0.0.1:\n",
      "      Successfully uninstalled gym-examples-0.0.1\n",
      "  Running setup.py develop for gym-examples\n",
      "Successfully installed gym-examples-0.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1683768523611,
     "user": {
      "displayName": "DEL Lab",
      "userId": "05361345683290158702"
     },
     "user_tz": 360
    },
    "id": "M0FpJmvmOgbY",
    "outputId": "3094b206-e004-44d5-fe75-9dbd8ddc3679"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/trithankar-mittra/Desktop/NLP_RL_DELLAB/gym-examples\n",
      "/home/trithankar-mittra/Desktop/NLP_RL_DELLAB\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "%cd gym-examples\n",
    "import gym_examples\n",
    "%cd ..\n",
    "import numpy as np\n",
    "\n",
    "episodes=2\n",
    "env = gym.make('gym_examples/RlNlpWorld-v0',render_mode=\"rgb_array\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qv5AAhIaOgbY"
   },
   "source": [
    "### Random Policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1683768523612,
     "user": {
      "displayName": "DEL Lab",
      "userId": "05361345683290158702"
     },
     "user_tz": 360
    },
    "id": "61MYl-tUOgbZ",
    "outputId": "09b876bc-4fef-4f96-f8fd-ead064c3c30c"
   },
   "outputs": [],
   "source": [
    "def policyR(observation):\n",
    "    return np.random.randint(0,6)\n",
    "\n",
    "dbg=True\n",
    "for _ in range(episodes):\n",
    "    cumulative_reward,steps=0,0\n",
    "    observation = env.reset(seed=42)\n",
    "    break\n",
    "    cnt=0\n",
    "    while True:\n",
    "        action = policyR(observation)  # User-defined policy function\n",
    "        observation, reward, terminated, info = env.step(action)\n",
    "        cumulative_reward+=reward\n",
    "        steps+=1\n",
    "        if dbg==True:\n",
    "            print(f'cumulative_reward {cumulative_reward}; action {action}')\n",
    "            break\n",
    "        if terminated:\n",
    "            break\n",
    "    print(f'Cumulative Reward ~ {cumulative_reward}; TimeTaken ~ {steps}')\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Human Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def policyH(observation):\n",
    "    ch=int(input())\n",
    "    return ch\n",
    "\n",
    "dbg=True\n",
    "for _ in range(episodes):\n",
    "    cumulative_reward,steps=0,0\n",
    "    observation = env.reset(seed=42)\n",
    "    cnt=0\n",
    "    while True:\n",
    "        action = policyH(observation)  # User-defined policy function\n",
    "        observation, reward, terminated, info = env.step(action)\n",
    "        cumulative_reward+=reward\n",
    "        steps+=1\n",
    "        if dbg==True:\n",
    "            print(f'cumulative_reward {cumulative_reward}; action {action}')\n",
    "            break\n",
    "        if terminated:\n",
    "            break\n",
    "    print(f'Cumulative Reward ~ {cumulative_reward}; TimeTaken ~ {steps}')\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BgkqTy7qaOF3"
   },
   "source": [
    "### Delete Files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1683768523612,
     "user": {
      "displayName": "DEL Lab",
      "userId": "05361345683290158702"
     },
     "user_tz": 360
    },
    "id": "QKhpu7uBaM8J",
    "outputId": "c75adda0-374e-488a-85ec-13d679d1fe23"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def delete(state):\n",
    "    file_path=f'/content/drive/MyDrive/RL_NLP/save_state/{state}.json'\n",
    "    print(state)\n",
    "    U.delete(state)\n",
    "    try:\n",
    "        os.unlink(file_path)\n",
    "    except Exception as e:\n",
    "        print(f'Error deleting {file_path}: {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IOU1LxNUOgbZ"
   },
   "source": [
    "### Q-Learning w/o NLP. (NAIVE MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1224,
     "status": "ok",
     "timestamp": 1683768524832,
     "user": {
      "displayName": "DEL Lab",
      "userId": "05361345683290158702"
     },
     "user_tz": 360
    },
    "id": "f6WX6YpyOgba",
    "outputId": "2fe94d59-65d8-43e0-88be-7aa7ab9a237c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘/content/drive/MyDrive/RL_NLP/results’: File exists\n"
     ]
    }
   ],
   "source": [
    "!rm -rf \"{PROJECT_PATH}/save_state\"\n",
    "!mkdir \"{PROJECT_PATH}/save_state\"\n",
    "!mkdir \"{PROJECT_PATH}/results\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 326,
     "status": "ok",
     "timestamp": 1683768598997,
     "user": {
      "displayName": "DEL Lab",
      "userId": "05361345683290158702"
     },
     "user_tz": 360
    },
    "id": "I9I2zokVhaOS",
    "outputId": "ad7a8b15-8f51-4618-dd3a-05099e43e0ef"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('gym_examples/RlNlpWorld-v0',mx_timeSteps=100,render_mode=\"rgb_array\")\n",
    "num_actions=6\n",
    "update_after_actions=2**8\n",
    "update_target_network=2**10\n",
    "frame_cnt,epsilon_greedy=0,20000\n",
    "episodes,mxSteps=100000,100\n",
    "episode_reward_hist,episode_avg_reward=[],[]\n",
    "epsilon,epsilon_min,epsilon_max=1.0,0.005,1.0 \n",
    "epsilon_interval = (\n",
    "    epsilon_max - epsilon_min\n",
    ") \n",
    "epsilon_greedy_frames = 100000.0\n",
    "batch_size=2**7\n",
    "action=-1\n",
    "replay_mem={'state':[],'next_state':[],'action':[],'reward':[],'small_n_state':[]}\n",
    "mx_replay_memory=100000\n",
    "gamma=0.9\n",
    "# MODEL\n",
    "save_step=5000;load_from_checkpoint=False\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "epochs=10\n",
    "first=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 336,
     "status": "ok",
     "timestamp": 1683768589713,
     "user": {
      "displayName": "DEL Lab",
      "userId": "05361345683290158702"
     },
     "user_tz": 360
    },
    "id": "jsWmaOEPDBiJ",
    "outputId": "1ad9c965-cc25-4bae-b27d-d9d2cbbfef17"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "episodes=1000\n",
    "load_from_checkpoint=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ilhGsmWOCEvP"
   },
   "source": [
    "#### Run this cell multiple times in short episodes because google colab throws error (disconnects with drive) if run for long.\n",
    "Keep the cnt here -> 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "Kx_noYF2Ogba"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:190: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.\u001b[0m\n",
      "  logger.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:165: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n",
      "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:227: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
      "  logger.deprecation(\n",
      "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:165: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n",
      "/content/drive/MyDrive/RL_NLP/model.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  reward_true=torch.tensor(reward_true)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:0 DQN_Loss:2.5 IMG_Loss:-1\n",
      "Train Epoch:1 DQN_Loss:2.5 IMG_Loss:-1\n",
      "Updating N/W ...30720\n",
      "Updating N/W ...31744\n",
      "Train Epoch:100 DQN_Loss:2.5234375 IMG_Loss:-1\n",
      "Train Epoch:101 DQN_Loss:2.5234375 IMG_Loss:-1\n",
      "Updating N/W ...32768\n",
      "Updating N/W ...33792\n",
      "Updating N/W ...34816\n",
      "Saving ...\n",
      "Train Epoch:200 DQN_Loss:1.7578125 IMG_Loss:-1\n",
      "Train Epoch:201 DQN_Loss:1.7578125 IMG_Loss:-1\n",
      "Updating N/W ...35840\n",
      "Updating N/W ...36864\n",
      "Train Epoch:300 DQN_Loss:5.625 IMG_Loss:-1\n",
      "Train Epoch:301 DQN_Loss:5.625 IMG_Loss:-1\n",
      "Updating N/W ...37888\n",
      "Updating N/W ...38912\n",
      "Updating N/W ...39936\n",
      "Saving ...\n",
      "Train Epoch:400 DQN_Loss:1.765625 IMG_Loss:-1\n",
      "Train Epoch:401 DQN_Loss:1.765625 IMG_Loss:-1\n",
      "Updating N/W ...40960\n",
      "Updating N/W ...41984\n",
      "Train Epoch:500 DQN_Loss:2.53125 IMG_Loss:-1\n",
      "Train Epoch:501 DQN_Loss:2.53125 IMG_Loss:-1\n",
      "Updating N/W ...43008\n",
      "Updating N/W ...44032\n",
      "Saving ...\n",
      "Updating N/W ...45056\n",
      "Train Epoch:600 DQN_Loss:1.765625 IMG_Loss:-1\n",
      "Train Epoch:601 DQN_Loss:1.765625 IMG_Loss:-1\n",
      "Updating N/W ...46080\n",
      "Updating N/W ...47104\n",
      "Train Epoch:700 DQN_Loss:0.984375 IMG_Loss:-1\n",
      "Train Epoch:701 DQN_Loss:0.984375 IMG_Loss:-1\n",
      "Updating N/W ...48128\n"
     ]
    }
   ],
   "source": [
    "model=M.NNModel().to(device)\n",
    "model_traget=M.NNModel().to(device)\n",
    "if load_from_checkpoint==True:\n",
    "    model.load_state_dict(torch.load('results/model.ml'))\n",
    "    model_traget.load_state_dict(torch.load('results/model.ml'))\n",
    "    episode_reward_hist=np.load(f'results/ep_tot.npy').tolist()\n",
    "    episode_avg_reward=np.load(f'results/ep_avg.npy').tolist()\n",
    "    replay_mem['state']=np.load(f'results/state.npy').tolist()\n",
    "    replay_mem['next_state']=np.load(f'results/next_state.npy').tolist()\n",
    "    # replay_mem['small_n_state']=np.load(f'results/small_n_state.npy').tolist()\n",
    "    replay_mem['action']=np.load(f'results/action.npy').tolist()\n",
    "    replay_mem['reward']=np.load(f'results/reward.npy').tolist()\n",
    "    file_read=open('results/frame_cnt.dat','r')\n",
    "    frame_cnt=int(file_read.read())\n",
    "    file_read.close()\n",
    "    file_read=open('results/epsilon.dat','r')\n",
    "    epsilon=float(file_read.read())\n",
    "    file_read.close()\n",
    "if first:   \n",
    "    load_from_checkpoint=True\n",
    "    first=False\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.5)\n",
    "# frame_cnt\n",
    "# epsilon\n",
    "for _ in range(episodes):\n",
    "    episode_reward=0\n",
    "    state = env.reset(seed=42)\n",
    "    for __ in range(mxSteps):\n",
    "        frame_cnt+=1\n",
    "        epsilon -= epsilon_interval / epsilon_greedy_frames\n",
    "        epsilon = max(epsilon, epsilon_min)\n",
    "        U.phi(state);replay_mem['state'].append(U.save(state))\n",
    "        if frame_cnt<=epsilon_greedy or epsilon>np.random.rand(1)[0]:\n",
    "            action=np.random.choice(num_actions)\n",
    "            # with open('my_dict.json', 'w') as f:\n",
    "            #     U.phi(state)\n",
    "            #     state[\"visual\"]=state[\"visual\"].tolist()\n",
    "            #     json.dump(state,f)\n",
    "        else:\n",
    "            action=(M.predict(model,[U.get(replay_mem['state'][-1])],device))[1]\n",
    "            action=action.item()\n",
    "        next_state,reward,terminated,info=env.step(action)\n",
    "        episode_reward+=reward \n",
    "        # temp_next_state=copy.deepcopy(next_state) #to save small image.\n",
    "        state=copy.deepcopy(next_state)\n",
    "        U.phi(next_state);replay_mem['next_state'].append(U.save(next_state))\n",
    "        # U.phiXtra(temp_next_state);replay_mem['small_n_state'].append(U.save(temp_next_state))\n",
    "        replay_mem['action'].append(action)\n",
    "        replay_mem['reward'].append(reward)\n",
    "        if (frame_cnt%save_step==0): # SAVE model temporarily so progress is not lost.\n",
    "            print(f'Saving ...')\n",
    "            np.save(f'results/ep_tot.npy',np.array(episode_reward_hist))\n",
    "            np.save(f'results/ep_avg.npy',np.array(episode_avg_reward))\n",
    "            np.save(f'results/state.npy',np.array(replay_mem['state']))\n",
    "            np.save(f'results/next_state.npy',np.array(replay_mem['next_state']))\n",
    "            # np.save(f'results/small_n_state.npy',np.array(replay_mem['small_n_state']))\n",
    "            np.save(f'results/action.npy',np.array(replay_mem['action']))\n",
    "            np.save(f'results/reward.npy',np.array(replay_mem['reward']))\n",
    "            file_write=open('results/frame_cnt.dat','w')\n",
    "            file_write.write(str(frame_cnt))\n",
    "            file_write.close()\n",
    "            file_write=open('results/epsilon.dat','w')\n",
    "            file_write.write(str(epsilon))\n",
    "            file_write.close()\n",
    "            torch.save(model.state_dict(),'results/model.ml')\n",
    "        if frame_cnt%update_after_actions==0 and len(replay_mem['reward'])>batch_size:\n",
    "            choices=np.random.choice( range( len(replay_mem['reward']) ), size=batch_size )\n",
    "            STATE=[U.get(replay_mem['state'][id]) for id in choices]\n",
    "            NEXT_STATE=[U.get(replay_mem['next_state'][id]) for id in choices]\n",
    "            # SMALL_NEXT_STATE=[U.get(replay_mem['small_n_state'][id]) for id in choices]\n",
    "            ACTION=torch.stack([U.oneHot(num_actions,replay_mem['action'][id]).squeeze() for id in choices])\n",
    "            REWARD=np.array([replay_mem['reward'][id] for id in choices])\n",
    "            IMG=np.array([_state_[\"visual\"] for _state_ in STATE])\n",
    "            TEMP=M.predict(model_traget,NEXT_STATE,device)\n",
    "            REWARD_T=torch.tensor(REWARD)\n",
    "            if torch.cuda.is_available():\n",
    "                REWARD_T=REWARD_T.to(torch.device(\"cuda:0\")) \n",
    "            reward_true=torch.add(REWARD_T,gamma*TEMP[0][0])\n",
    "            for epoch in range(epochs):\n",
    "                # M.train(model,reward_true,STATE,SMALL_NEXT_STATE,ACTION,device,optimizer,type='dqn')\n",
    "                # M.train(model,reward_true,STATE,SMALL_NEXT_STATE,ACTION,device,optimizer,type='image')\n",
    "                M.train(model,reward_true,STATE,ACTION,device,optimizer,type='dqn')\n",
    "                M.train(model,reward_true,STATE,ACTION,device,optimizer,type='image')\n",
    "        if frame_cnt%update_target_network==0:\n",
    "            print(f'Updating N/W ...{frame_cnt}')\n",
    "            model_traget.load_state_dict(model.state_dict())\n",
    "        if len(replay_mem['reward'])>mx_replay_memory:\n",
    "            print('Deleting ...')\n",
    "            delete(replay_mem['state'][:1][0]);del replay_mem['state'][:1]\n",
    "            # delete(replay_mem['small_n_state'][:1][0]);del replay_mem['small_n_state'][:1]\n",
    "            delete(replay_mem['next_state'][:1][0]);del replay_mem['next_state'][:1]\n",
    "            del replay_mem['action'][:1]\n",
    "            del replay_mem['reward'][:1]\n",
    "        if terminated:\n",
    "            break\n",
    "    episode_reward_hist.append(episode_reward)\n",
    "    episode_avg_reward.append(episode_reward/__)\n",
    "\n",
    "# Save Relevant Results\n",
    "np.save('results/ep_tot.npy',np.array(episode_reward_hist))\n",
    "np.save('results/ep_avg.npy',np.array(episode_avg_reward))\n",
    "torch.save(model.state_dict(),'results/model.ml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P5Bsa8n6Ogbc"
   },
   "outputs": [],
   "source": [
    "# Loading Results.\n",
    "episode_reward_hist=np.load('results/ep_tot.npy')\n",
    "episode_avg_reward=np.load('results/ep_avg.npy')\n",
    "U.plot(episode_reward_hist,'Total Reward','Total Reward V/S Episodes')\n",
    "U.plot(episode_avg_reward,'Avg. Reward','Avg. Reward Per Step V/S Episodes')\n",
    "model_tr=M.NNModel().to(device)\n",
    "model_tr.load_state_dict(torch.load('results/model.ml'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2fWimtLykKvE"
   },
   "source": [
    "### Q-Learning with NLP Fully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K-jR_OyREhiE"
   },
   "outputs": [],
   "source": [
    "!rm -rf \"{PROJECT_PATH}/save_state\"\n",
    "!mkdir \"{PROJECT_PATH}/save_state\"\n",
    "!mkdir \"{PROJECT_PATH}/results\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cSiJ8u46l2qK"
   },
   "outputs": [],
   "source": [
    "import model_nlp as M_NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_lkqyIsYZWsK"
   },
   "outputs": [],
   "source": [
    "num_actions=6\n",
    "update_after_actions=2**8\n",
    "update_target_network=2**10\n",
    "frame_cnt,epsilon_greedy=0,20000\n",
    "episodes,mxSteps=100000,100\n",
    "episode_reward_hist,episode_avg_reward=[],[]\n",
    "epsilon,epsilon_min,epsilon_max=1.0,0.005,1.0 \n",
    "epsilon_interval = (\n",
    "    epsilon_max - epsilon_min\n",
    ") \n",
    "epsilon_greedy_frames = 100000.0\n",
    "batch_size=2**7\n",
    "action=-1\n",
    "replay_mem={'state':[],'next_state':[],'action':[],'reward':[],'small_n_state':[]}\n",
    "mx_replay_memory=100000\n",
    "gamma=0.9\n",
    "# MODEL\n",
    "save_step=5000;load_from_checkpoint=False\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "epochs=10\n",
    "first=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XPCeRTXaZXcz"
   },
   "outputs": [],
   "source": [
    "episodes=1000\n",
    "load_from_checkpoint=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ShEaaovxkHpW"
   },
   "outputs": [],
   "source": [
    "model=M_NLP.NNModelNLP().to(device)\n",
    "model_traget=M_NLP.NNModelNLP().to(device)\n",
    "if load_from_checkpoint==True:\n",
    "    model.load_state_dict(torch.load('results/model_nlpf.ml'))\n",
    "    model_traget.load_state_dict(torch.load('results/model_nlpf.ml'))\n",
    "    episode_reward_hist=np.load(f'results/ep_tot_nlpf.npy').tolist()\n",
    "    episode_avg_reward=np.load(f'results/ep_avg_nlpf.npy').tolist()\n",
    "    replay_mem['state']=np.load(f'results/state_nlpf.npy').tolist()\n",
    "    replay_mem['next_state']=np.load(f'results/next_state_nlpf.npy').tolist()\n",
    "    replay_mem['action']=np.load(f'results/action_nlpf.npy').tolist()\n",
    "    replay_mem['reward']=np.load(f'results/reward_nlpf.npy').tolist()\n",
    "    file_read=open('results/frame_cnt_nlpf.dat','r')\n",
    "    frame_cnt=int(file_read.read())\n",
    "    file_read.close()\n",
    "    file_read=open('results/epsilon_nlpf.dat','r')\n",
    "    epsilon=float(file_read.read())\n",
    "    file_read.close()\n",
    "if first:   \n",
    "    load_from_checkpoint=True\n",
    "    first=False\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.5)\n",
    "\n",
    "for _ in range(episodes):\n",
    "    episode_reward=0\n",
    "    state = env.reset(seed=42)\n",
    "    print(f'Episode no {_}')\n",
    "    for __ in range(mxSteps):\n",
    "        frame_cnt+=1\n",
    "        epsilon -= epsilon_interval / epsilon_greedy_frames\n",
    "        epsilon = max(epsilon, epsilon_min)\n",
    "        U.phi(state);replay_mem['state'].append(U.save(state))\n",
    "        if frame_cnt<=epsilon_greedy or epsilon>np.random.rand(1)[0]:\n",
    "            action=np.random.choice(num_actions)\n",
    "        else:\n",
    "            action=(M_NLP.predict(model,[U.get(replay_mem['state'][-1])],device))[1]\n",
    "            action=action.item()\n",
    "        next_state,reward,terminated,info=env.step(action)\n",
    "        episode_reward+=reward \n",
    "        # temp_next_state=copy.deepcopy(next_state) #to save small image.\n",
    "        state=copy.deepcopy(next_state)\n",
    "        U.phi(next_state);replay_mem['next_state'].append(U.save(next_state))\n",
    "        # U.phiXtra(temp_next_state);replay_mem['small_n_state'].append(U.save(temp_next_state))\n",
    "        replay_mem['action'].append(action)\n",
    "        replay_mem['reward'].append(reward)\n",
    "        if (frame_cnt%save_step==0): # SAVE model temporarily so progress is not lost.\n",
    "            print(f'Saving ...')\n",
    "            np.save(f'results/ep_tot_nlpf.npy',np.array(episode_reward_hist))\n",
    "            np.save(f'results/ep_avg_nlpf.npy',np.array(episode_avg_reward))\n",
    "            np.save(f'results/state_nlpf.npy',np.array(replay_mem['state']))\n",
    "            np.save(f'results/next_state_nlpf.npy',np.array(replay_mem['next_state']))\n",
    "            np.save(f'results/action_nlpf.npy',np.array(replay_mem['action']))\n",
    "            np.save(f'results/reward_nlpf.npy',np.array(replay_mem['reward']))\n",
    "            file_write=open('results/frame_cnt_nlpf.dat','w')\n",
    "            file_write.write(str(frame_cnt))\n",
    "            file_write.close()\n",
    "            file_write=open('results/epsilon_nlpf.dat','w')\n",
    "            file_write.write(str(epsilon))\n",
    "            file_write.close()\n",
    "            torch.save(model.state_dict(),'results/model_nlpf.ml')\n",
    "        if frame_cnt%update_after_actions==0 and len(replay_mem['reward'])>batch_size:\n",
    "            choices=np.random.choice( range( len(replay_mem['reward']) ), size=batch_size )\n",
    "            STATE=[U.get(replay_mem['state'][id]) for id in choices]\n",
    "            NEXT_STATE=[U.get(replay_mem['next_state'][id]) for id in choices]\n",
    "            # SMALL_NEXT_STATE=[U.get(replay_mem['small_n_state'][id]) for id in choices]\n",
    "            ACTION=torch.stack([U.oneHot(num_actions,replay_mem['action'][id]).squeeze() for id in choices])\n",
    "            REWARD=np.array([replay_mem['reward'][id] for id in choices])\n",
    "            IMG=np.array([_state_[\"visual\"] for _state_ in STATE])\n",
    "            TEMP=M_NLP.predict(model_traget,NEXT_STATE,device);\n",
    "            REWARD_T=torch.tensor(REWARD)\n",
    "            if torch.cuda.is_available():\n",
    "                REWARD_T=REWARD_T.to(torch.device(\"cuda:0\")) \n",
    "            reward_true=torch.add(REWARD_T,gamma*TEMP[0][0])\n",
    "            for epoch in range(epochs):\n",
    "                # M_NLP.train(model,reward_true,STATE,SMALL_NEXT_STATE,ACTION,device,optimizer,type='dqn')\n",
    "                # M_NLP.train(model,reward_true,STATE,SMALL_NEXT_STATE,ACTION,device,optimizer,type='image')\n",
    "                M_NLP.train(model,reward_true,STATE,ACTION,device,optimizer,type='dqn')\n",
    "                M_NLP.train(model,reward_true,STATE,ACTION,device,optimizer,type='image')\n",
    "        if frame_cnt%update_target_network==0:\n",
    "            model_traget.load_state_dict(model.state_dict())\n",
    "        if len(replay_mem['reward'])>mx_replay_memory:\n",
    "            delete(replay_mem['state'][:1][0]);del replay_mem['state'][:1]\n",
    "            # delete(replay_mem['small_n_state'][:1][0]);del replay_mem['small_n_state'][:1]\n",
    "            delete(replay_mem['next_state'][:1][0]);del replay_mem['next_state'][:1]\n",
    "            del replay_mem['action'][:1]\n",
    "            del replay_mem['reward'][:1]\n",
    "        if terminated:\n",
    "            break\n",
    "    \n",
    "    episode_reward_hist.append(episode_reward)\n",
    "    episode_avg_reward.append(episode_reward/__)\n",
    "\n",
    "# Save Relevant Results\n",
    "np.save('results/ep_tot_nlpf.npy',np.array(episode_reward_hist))\n",
    "np.save('results/ep_avg_nlpf.npy',np.array(episode_avg_reward))\n",
    "torch.save(model.state_dict(),'results/model_nlpf.ml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V7nJNTjx4_gB"
   },
   "source": [
    "### Q-Learning with NLP. {TBD}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7-qRAsmDji6c"
   },
   "outputs": [],
   "source": [
    "num_actions=6\n",
    "update_after_actions=2**8\n",
    "update_target_network=2**10\n",
    "frame_cnt,epsilon_greedy=0,20000\n",
    "episodes,mxSteps=100000,100\n",
    "episode_reward_hist,episode_avg_reward=[],[]\n",
    "epsilon,epsilon_min,epsilon_max=1.0,0.005,1.0 \n",
    "epsilon_interval = (\n",
    "    epsilon_max - epsilon_min\n",
    ") \n",
    "epsilon_greedy_frames = 100000.0\n",
    "batch_size=2**7\n",
    "action=-1\n",
    "replay_mem={'state':[],'next_state':[],'action':[],'reward':[],'small_n_state':[]}\n",
    "mx_replay_memory=100000\n",
    "gamma=0.9\n",
    "# MODEL\n",
    "save_step=5000;load_from_checkpoint=False\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "epochs=10\n",
    "first=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W1QZzYbCjmL9"
   },
   "outputs": [],
   "source": [
    "episodes=1000\n",
    "load_from_checkpoint=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J56y_01u44b6"
   },
   "outputs": [],
   "source": [
    "model=M_NLP.NNModelNLP().to(device)\n",
    "model_traget=M_NLP.NNModelNLP().to(device)\n",
    "if load_from_checkpoint==True:\n",
    "    model.load_state_dict(torch.load('results/model_nlp.ml'))\n",
    "    model_traget.load_state_dict(torch.load('results/model_nlp.ml'))\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.5)\n",
    "epochs=10\n",
    "#### REMOVE INSTRUCTIONS SLOWLY MODEL IS NOT SO DEPENDENT ####\n",
    "epsilon_nlp=1.0\n",
    "epsilon_greedy_frames_nlp=100000.0\n",
    "##############################################################\n",
    "\n",
    "for _ in range(episodes):\n",
    "    episode_reward=0\n",
    "    state = env.reset(seed=42)\n",
    "    for __ in range(mxSteps):\n",
    "        frame_cnt+=1\n",
    "        epsilon -= epsilon_interval / epsilon_greedy_frames\n",
    "        epsilon = max(epsilon, epsilon_min)\n",
    "#### REMOVE INSTRUCTIONS SLOWLY MODEL IS NOT SO DEPENDENT ####\n",
    "        epsilon_nlp -= epsilon_interval / epsilon_greedy_frames_nlp\n",
    "        if epsilon_nlp<np.random.rand(1)[0]:\n",
    "            state['text']='[PAD]'\n",
    "##############################################################\n",
    "        U.phi(state);replay_mem['state'].append(U.save(state))\n",
    "        if frame_cnt<=epsilon_greedy or epsilon>np.random.rand(1)[0]:\n",
    "            action=np.random.choice(num_actions)\n",
    "        else:\n",
    "            action=(M_NLP.predict(model,[U.get(replay_mem['state'][-1])],device))[1]\n",
    "            action=action.item()\n",
    "        next_state,reward,terminated,info=env.step(action)\n",
    "        episode_reward+=reward \n",
    "        temp_next_state=copy.deepcopy(next_state) #to save small image.\n",
    "        state=copy.deepcopy(next_state)\n",
    "        U.phi(next_state);replay_mem['next_state'].append(U.save(next_state))\n",
    "        U.phiXtra(temp_next_state);replay_mem['small_n_state'].append(U.save(temp_next_state))\n",
    "        replay_mem['action'].append(action)\n",
    "        replay_mem['reward'].append(reward)\n",
    "        if (_!=0 and _%save_step==0): # SAVE model temporarily so progress is not lost.\n",
    "            np.save(f'results/ep_tot_nlp{_}.npy',np.array(episode_reward_hist))\n",
    "            np.save(f'results/ep_avg_nlp{_}.npy',np.array(episode_avg_reward))\n",
    "            torch.save(model.state_dict(),'results/model_nlp.ml')\n",
    "        if frame_cnt%update_after_actions==0 and len(replay_mem['reward'])>batch_size:\n",
    "            choices=np.random.choice( range( len(replay_mem['reward']) ), size=batch_size )\n",
    "            STATE=[U.get(replay_mem['state'][id]) for id in choices]\n",
    "            NEXT_STATE=[U.get(replay_mem['next_state'][id]) for id in choices]\n",
    "            SMALL_NEXT_STATE=[U.get(replay_mem['small_n_state'][id]) for id in choices]\n",
    "            ACTION=torch.stack([U.oneHot(num_actions,replay_mem['action'][id]).squeeze() for id in choices])\n",
    "            REWARD=np.array([replay_mem['reward'][id] for id in choices])\n",
    "            IMG=np.array([_state_[\"visual\"] for _state_ in STATE])\n",
    "            TEMP=M_NLP.predict(model_traget,NEXT_STATE,device);\n",
    "            REWARD_T=torch.tensor(REWARD)\n",
    "            reward_true=torch.add(REWARD_T,gamma*TEMP[0][0])\n",
    "            for epoch in range(epochs):\n",
    "                M_NLP.train(model,reward_true,STATE,SMALL_NEXT_STATE,ACTION,device,optimizer,type='dqn')\n",
    "                M_NLP.train(model,reward_true,STATE,SMALL_NEXT_STATE,ACTION,device,optimizer,type='image')\n",
    "        if frame_cnt%update_target_network==0:\n",
    "            model_traget.load_state_dict(model.state_dict())\n",
    "        if len(replay_mem['reward'])>mx_replay_memory:\n",
    "            delete(replay_mem['state'][:1][0]);del replay_mem['state'][:1]\n",
    "            delete(replay_mem['small_n_state'][:1][0]);del replay_mem['small_n_state'][:1]\n",
    "            delete(replay_mem['next_state'][:1][0]);del replay_mem['next_state'][:1]\n",
    "            del replay_mem['action'][:1]\n",
    "            del replay_mem['reward'][:1]\n",
    "        if terminated:\n",
    "            break\n",
    "    \n",
    "    episode_reward_hist.append(episode_reward)\n",
    "    episode_avg_reward.append(episode_reward/__)\n",
    "\n",
    "# Save Relevant Results\n",
    "np.save('results/ep_tot_nlp.npy',np.array(episode_reward_hist))\n",
    "np.save('results/ep_avg_nlp.npy',np.array(episode_avg_reward))\n",
    "torch.save(model.state_dict(),'results/model_nlp.ml')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "name": "",
   "version": ""
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "607977446fd7db0f3e2ddba643f6a09248589e32f4e48c8af2325eb4debcc38d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
